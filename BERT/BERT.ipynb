{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5c225d-637d-42bb-b4a9-098bd27cb486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-22 04:09:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
      "\n",
      "2025-02-22 04:09:17 (146 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf443f0c-36e2-4ec0-b591-897d434a881e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming your notebook's working directory is set such that ../llm-tokenizer is reachable:\n",
    "tokenizer_dir = os.path.abspath(os.path.join(os.getcwd(), '../llm-tokenizer'))\n",
    "if tokenizer_dir not in sys.path:\n",
    "    sys.path.insert(0, tokenizer_dir)\n",
    "\n",
    "import BPETokenizer  # Now you should be able to import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22820cf2-f6be-4a2b-84cb-d7dc0cd42490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device index: 0\n",
      "Running on GPU: NVIDIA RTX A6000\n",
      "GPU properties:\n",
      "  - Compute Capability: 8.6\n",
      "  - Total Memory: 47.54 GB\n",
      "  - Multiprocessor Count: 84\n",
      "  - Max Threads per Multiprocessor: 1536\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    device_props = torch.cuda.get_device_properties(current_device)\n",
    "    memory_summary = torch.cuda.memory_summary(device=current_device, abbreviated=True)\n",
    "    \n",
    "    print(\"Current device index:\", current_device)\n",
    "    print(\"Running on GPU:\", device_name)\n",
    "    print(\"GPU properties:\")\n",
    "    print(\"  - Compute Capability:\", f\"{device_props.major}.{device_props.minor}\")\n",
    "    print(\"  - Total Memory:\", f\"{device_props.total_memory / (1024**3):.2f} GB\")\n",
    "    print(\"  - Multiprocessor Count:\", device_props.multi_processor_count)\n",
    "    print(\"  - Max Threads per Multiprocessor:\", device_props.max_threads_per_multi_processor)\n",
    "else:\n",
    "    print(\"CUDA is not available, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b04823-54d4-47dd-a2f3-fc99240d0f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        len(text)\n",
    "\n",
    "        tokenizer = BPETokenizer.Tokenizer(text, encoding_vocab_size=2000, raw_tokens=False)\n",
    "        tokenizer.load_from_file()\n",
    "        encoded_dataset = tokenizer.encode(text, raw_tokens=False)\n",
    "        print(f\"max vocabulary size={max(encoded_dataset)}, compression ratio={len(encoded_dataset) / len(text)}\")\n",
    "        split = int(len(encoded_dataset) * 0.80)\n",
    "        self.train_data =  torch.tensor(encoded_dataset[:split])\n",
    "        self.val_data = torch.tensor(encoded_dataset[split+config.block_size:])\n",
    "        print(f\"train_data.shape={self.train_data.shape}, val_data.shape={self.val_data.shape}\")\n",
    "        \n",
    "        self.train_data_ix = 0\n",
    "        self.val_data_ix = 0\n",
    "        self.batch_step = self.config.batch_size * self.config.block_size \n",
    "        \n",
    "    def next_batch(self, mode=\"train\", device=device):\n",
    "        \"\"\" mode=[\"train\", \"eval\"] \"\"\"\n",
    "        if mode == \"train\":\n",
    "            x, y = self._next_batch_train()\n",
    "        else:\n",
    "            x, y = self._next_batch_eval()\n",
    "        return x.to(device), y.to(device)\n",
    "    \n",
    "    def _next_batch_train(self):\n",
    "        \n",
    "        data = self.train_data\n",
    "        ix = self.train_data_ix \n",
    "        \n",
    "        buf = data[ix:ix+self.batch_step + 1]     \n",
    "        x = buf[:-1].view(self.config.batch_size, self.config.block_size)\n",
    "        y = buf[1:].view(self.config.batch_size, self.config.block_size)\n",
    "        \n",
    "        self.train_data_ix += self.batch_step \n",
    "        if self.train_data_ix + self.batch_step + 1 > len(self.train_data):\n",
    "            self.train_data_ix = 0\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def _next_batch_eval(self):\n",
    "        \n",
    "        data = self.val_data\n",
    "        ix = self.val_data_ix \n",
    "        \n",
    "        buf = data[ix:ix+self.batch_step + 1]     \n",
    "        x = buf[:-1].view(self.config.batch_size, self.config.block_size)\n",
    "        y = buf[1:].view(self.config.batch_size, self.config.block_size)\n",
    "        \n",
    "        self.val_data_ix += self.batch_step \n",
    "        if self.val_data_ix + self.batch_step + 1 > len(self.val_data):\n",
    "            self.val_data_ix = 0\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72adc4f3-2640-4a05-9969-079806cbcb12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BERTConfig:\n",
    "    batch_size = 32\n",
    "    block_size = 10\n",
    "    \n",
    "config = BERTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c2b8ed-11e0-485e-9bcc-b28bcd8a0471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max vocabulary size=2213, compression ratio=0.4458684554516162\n",
      "train_data.shape=torch.Size([397855]), val_data.shape=torch.Size([99454])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(config)\n",
    "x, y = data_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79332896-fd09-4b61-999b-932d87e2beb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10]), torch.Size([32, 10]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570a1fe-3a81-44bc-958e-ff637e12288c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
