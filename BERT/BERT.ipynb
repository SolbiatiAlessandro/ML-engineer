{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5c225d-637d-42bb-b4a9-098bd27cb486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-22 04:09:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
      "\n",
      "2025-02-22 04:09:17 (146 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf443f0c-36e2-4ec0-b591-897d434a881e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming your notebook's working directory is set such that ../llm-tokenizer is reachable:\n",
    "tokenizer_dir = os.path.abspath(os.path.join(os.getcwd(), '../llm-tokenizer'))\n",
    "if tokenizer_dir not in sys.path:\n",
    "    sys.path.insert(0, tokenizer_dir)\n",
    "\n",
    "import BPETokenizer  # Now you should be able to import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22820cf2-f6be-4a2b-84cb-d7dc0cd42490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device index: 0\n",
      "Running on GPU: NVIDIA RTX A6000\n",
      "GPU properties:\n",
      "  - Compute Capability: 8.6\n",
      "  - Total Memory: 47.54 GB\n",
      "  - Multiprocessor Count: 84\n",
      "  - Max Threads per Multiprocessor: 1536\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    device_props = torch.cuda.get_device_properties(current_device)\n",
    "    memory_summary = torch.cuda.memory_summary(device=current_device, abbreviated=True)\n",
    "    \n",
    "    print(\"Current device index:\", current_device)\n",
    "    print(\"Running on GPU:\", device_name)\n",
    "    print(\"GPU properties:\")\n",
    "    print(\"  - Compute Capability:\", f\"{device_props.major}.{device_props.minor}\")\n",
    "    print(\"  - Total Memory:\", f\"{device_props.total_memory / (1024**3):.2f} GB\")\n",
    "    print(\"  - Multiprocessor Count:\", device_props.multi_processor_count)\n",
    "    print(\"  - Max Threads per Multiprocessor:\", device_props.max_threads_per_multi_processor)\n",
    "else:\n",
    "    print(\"CUDA is not available, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f9b04823-54d4-47dd-a2f3-fc99240d0f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        len(text)\n",
    "\n",
    "        self.tokenizer = BPETokenizer.Tokenizer(text, encoding_vocab_size=2000, raw_tokens=False)\n",
    "        self.tokenizer.load_from_file()\n",
    "        encoded_dataset = self.tokenizer.encode(text, raw_tokens=False)\n",
    "        print(f\"max vocabulary size={max(encoded_dataset)}, compression ratio={len(encoded_dataset) / len(text)}\")\n",
    "        split = int(len(encoded_dataset) * 0.80)\n",
    "        self.train_data =  torch.tensor(encoded_dataset[:split])\n",
    "        self.val_data = torch.tensor(encoded_dataset[split+config.block_size:])\n",
    "        print(f\"train_data.shape={self.train_data.shape}, val_data.shape={self.val_data.shape}\")\n",
    "        \n",
    "        self.train_data_ix = 0\n",
    "        self.val_data_ix = 0\n",
    "        self.batch_step = self.config.batch_size * self.config.block_size \n",
    "        \n",
    "    def next_batch(self, mode=\"train\", device=device):\n",
    "        \"\"\" mode=[\"train\", \"eval\"] \"\"\"\n",
    "        if mode == \"train\":\n",
    "            x, y = self._next_batch_train()\n",
    "        else:\n",
    "            x, y = self._next_batch_eval()\n",
    "        if device:\n",
    "            return x.to(device), y.to(device)\n",
    "        return x, y\n",
    "    \n",
    "    def _next_batch_train(self):\n",
    "        \n",
    "        data = self.train_data\n",
    "        ix = int(random() * (len(data) - 2*self.batch_step))\n",
    "        \n",
    "        buf = data[ix:ix+self.batch_step + 1]     \n",
    "        x = buf[:-1].view(self.config.batch_size, self.config.block_size)\n",
    "        y = buf[1:].view(self.config.batch_size, self.config.block_size)\n",
    "        \n",
    "        self.train_data_ix += self.batch_step \n",
    "        if self.train_data_ix + self.batch_step + 1 > len(self.train_data):\n",
    "            self.train_data_ix = 0\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def _next_batch_eval(self):\n",
    "        \n",
    "        data = self.train_data\n",
    "        ix = int(random() * (len(data) - 2*self.batch_step))\n",
    "        \n",
    "        buf = data[ix:ix+self.batch_step + 1]     \n",
    "        x = buf[:-1].view(self.config.batch_size, self.config.block_size)\n",
    "        y = buf[1:].view(self.config.batch_size, self.config.block_size)\n",
    "        \n",
    "        self.val_data_ix += self.batch_step \n",
    "        if self.val_data_ix + self.batch_step + 1 > len(self.val_data):\n",
    "            self.val_data_ix = 0\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "72adc4f3-2640-4a05-9969-079806cbcb12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BERTConfig:\n",
    "    BERT_batch_size = 6\n",
    "    batch_size = BERT_batch_size * 2\n",
    "    block_size = 5\n",
    "    \n",
    "config = BERTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "83c2b8ed-11e0-485e-9bcc-b28bcd8a0471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max vocabulary size=2213, compression ratio=0.4458684554516162\n",
      "train_data.shape=torch.Size([397855]), val_data.shape=torch.Size([99459])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(config)\n",
    "x, y = data_loader.next_batch(device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "79332896-fd09-4b61-999b-932d87e2beb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 924, 1510,  256,  570,  503],\n",
       "         [ 264,  391,  334,  473,  272],\n",
       "         [ 291,  287,  798,  280,  290],\n",
       "         [ 117,  738,  288,  623,  116],\n",
       "         [ 452,  488, 1890, 1903,  296],\n",
       "         [ 272, 1410,  260,  119,  316],\n",
       "         [ 104,  684,  282,  763,  263],\n",
       "         [ 316,  602,   67,  491,  259],\n",
       "         [ 369,   97, 1903,  783,  413],\n",
       "         [ 343,  109,  686, 1933,  761],\n",
       "         [1268,  331, 1510,  256,  320],\n",
       "         [ 391,  317,  413, 1624,  261]]),\n",
       " tensor([[1510,  256,  570,  503,  264],\n",
       "         [ 391,  334,  473,  272,  291],\n",
       "         [ 287,  798,  280,  290,  117],\n",
       "         [ 738,  288,  623,  116,  452],\n",
       "         [ 488, 1890, 1903,  296,  272],\n",
       "         [1410,  260,  119,  316,  104],\n",
       "         [ 684,  282,  763,  263,  316],\n",
       "         [ 602,   67,  491,  259,  369],\n",
       "         [  97, 1903,  783,  413,  343],\n",
       "         [ 109,  686, 1933,  761, 1268],\n",
       "         [ 331, 1510,  256,  320,  391],\n",
       "         [ 317,  413, 1624,  261, 1001]]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db542249-a0b7-49d7-bb4d-bc55479a7981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c570a1fe-3a81-44bc-958e-ff637e12288c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  70,  299,  296,   32, 1709,  984,  655],\n",
       "         [ 538,  585,  111,  364,  424,  293,  110],\n",
       "         [ 265, 1199,  114,  368,  261, 1398,  272],\n",
       "         [ 690,  854,  331,  953,  275,  969,  987],\n",
       "         [ 798,  261,  854,  331,   10,   70,  299],\n",
       "         [ 296,   32, 1709,  581,  293,  309,  686]]),\n",
       " tensor([[ 299,  296,   32, 1709,  984,  655,  538],\n",
       "         [ 585,  111,  364,  424,  293,  110,  265],\n",
       "         [1199,  114,  368,  261, 1398,  272,  690],\n",
       "         [ 854,  331,  953,  275,  969,  987,  798],\n",
       "         [ 261,  854,  331,   10,   70,  299,  296],\n",
       "         [  32, 1709,  581,  293,  309,  686,   32]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0c03f17-772f-4743-81f5-feda2354f5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2215"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83a55ddc-926e-43fb-9e20-5a4fdfacb493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2216 2217 2218\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "364c0fdd-5ba2-4b1b-a4c6-8b727c14a2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERTDataLoader(DataLoader):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.max_vocab_size = max(self.tokenizer.encoding_map.values())\n",
    "        self.CLS =self.max_vocab_size + 1\n",
    "        self.SEP = self.CLS + 1\n",
    "        self.MASK = self.SEP + 1\n",
    "        print(f\"new tokens: {self.max_vocab_size}, {self.CLS}, {self.SEP}, {self.MASK}\")\n",
    "        \n",
    "    def next_batch(self, device=device, test=False):\n",
    "        _x, _ = super().next_batch(device=None)\n",
    "        x, y_MLM, y_NSP = [], [], []\n",
    "        assert len(_x) % 2 == 0, \"BERTDataLoader batch size should be % 2 == 0\"\n",
    "        print(_x)\n",
    "        for ix in range(int(len(_x) / 2)): \n",
    "            x0 = _x[ix].clone()\n",
    "            if random() < 0.5:\n",
    "                x1 = _x[ix+1].clone()\n",
    "                y_NSP.append(torch.tensor(1))\n",
    "            else:\n",
    "                __x, _ = super().next_batch(device=None)\n",
    "                x1 = __x[0].clone()\n",
    "                y_NSP.append(torch.tensor(0))\n",
    "            y_MLM.append(self._make_input(x0, x1))\n",
    "            x.append(self._make_input(self._mask(x0), self._mask(x1)))\n",
    "            \n",
    "        if test:\n",
    "            for i, xx in enumerate(x):\n",
    "                assert (yy[0][1:1+BERTConfig.block_size] == _x[i]).all()\n",
    "                assert (yy[0][1+BERTConfig.block_size+1:1+2*BERTConfig.block_size+1] == _x[i+1]).all(), _x[i+1]\n",
    "                print(f\"{i} TEST PASSED same sentence\")\n",
    "            \n",
    "        return torch.stack(x).to(device), torch.stack(y_MLM).to(device), torch.stack(y_NSP).to(device)\n",
    "    \n",
    "    def _make_input(self, x0, x1):\n",
    "        return torch.cat([\n",
    "            torch.tensor([self.CLS]), \n",
    "            x0,\n",
    "            torch.tensor([self.SEP]), \n",
    "            x1,\n",
    "            torch.tensor([self.SEP])\n",
    "        ])\n",
    "    \n",
    "    def _mask(self, x):\n",
    "        for i, v in enumerate(x):\n",
    "            if random() < 0.15:\n",
    "                r2 = random()\n",
    "                if r2 < 0.80:\n",
    "                    x[i] = data_loader.MASK\n",
    "                elif 0.80 <= r2 < 0.90:\n",
    "                    x[i] = int(random() * (self.max_vocab_size - 100))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c2612558-9301-434e-baf6-f32721af4ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max vocabulary size=2213, compression ratio=0.4458684554516162\n",
      "train_data.shape=torch.Size([397855]), val_data.shape=torch.Size([99459])\n",
      "new tokens: 2215, 2216, 2217, 2218\n"
     ]
    }
   ],
   "source": [
    "data_loader = BERTDataLoader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0dc9bfc4-e0b5-45ef-921b-cfba202dd917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 813, 1257,  274,  736, 1458],\n",
      "        [ 114,  109,  409,  257, 1199],\n",
      "        [ 108,  297,  380,  652,  277],\n",
      "        [1510,  256, 2125,  465,  260],\n",
      "        [ 369,  653,  565,  415,  258],\n",
      "        [ 264,  287,  111, 1257,  274],\n",
      "        [ 653,  573, 1673,  275,  329],\n",
      "        [ 533, 2125,  114,  100,  259],\n",
      "        [2125,  465,  260, 1673,  110],\n",
      "        [ 954, 1398,  114,  391,  269],\n",
      "        [ 328, 1316,  380, 1410,  422],\n",
      "        [ 104,  320,  391,  269,  594]])\n"
     ]
    }
   ],
   "source": [
    "x, y_MLM, y_NSP = data_loader.next_batch(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b334a767-8352-41ab-9482-d586b336b176",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2216,  813, 1257,  274,  736, 1458, 2217,  504,  487, 2218,  272,  306,\n",
       "         2217],\n",
       "        [2216,   56,  109, 2218,  257, 1199, 2217,  307,  319,  400,  282, 2218,\n",
       "         2217],\n",
       "        [2216,  108,  297,  380,  652,  277, 2217,  276, 1410,  282,  341,  264,\n",
       "         2217],\n",
       "        [2216, 1510,  256, 2125,  465,  260, 2217, 1215,  343,  827,  653,  573,\n",
       "         2217],\n",
       "        [2216,  369,  653,  565,  415,  258, 2217,  264,  287,  111, 1257,  274,\n",
       "         2217],\n",
       "        [2216,  264,  287, 2218, 1257,  274, 2217,  265,  365,  515,  261,  294,\n",
       "         2217]], device='cuda:0')"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "added056-e723-49b3-8e76-c98d8333fbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2216,  813, 1257,  274,  736, 1458, 2217,  504,  487,  367,  272,  306,\n",
       "         2217],\n",
       "        [2216,  114,  109,  409,  257, 1199, 2217,  307,  319,  400,  282, 1590,\n",
       "         2217],\n",
       "        [2216,  108,  297,  380,  652,  277, 2217,  276, 1410,  282,  341,  264,\n",
       "         2217],\n",
       "        [2216, 1510,  256, 2125,  465,  260, 2217, 1215,  343,  827,  653,  573,\n",
       "         2217],\n",
       "        [2216,  369,  653,  565,  415,  258, 2217,  264,  287,  111, 1257,  274,\n",
       "         2217],\n",
       "        [2216,  264,  287,  111, 1257,  274, 2217,  265,  365,  515,  261,  294,\n",
       "         2217]], device='cuda:0')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "659c73cb-45eb-4d8d-95c4-21deab1ce0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "35523754-f825-4384-80af-6f9215929df7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mstack(x), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "torch.stack(x), torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "63777b99-f51b-4605-a8f6-9b2daffc54eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([2216,  343,  347,  404,  827,   10, 2217, 1733,  256,  289, 2010,  709,\n",
       "          2217]),\n",
       "  False],\n",
       " [tensor([2216,   39,   83,  595,  383,  105, 2217,  573,  320, 1483,  111,   45,\n",
       "          2217]),\n",
       "  True],\n",
       " [tensor([2216,  573,  320, 1483,  111,   45, 2217, 1013,  102, 1004,  316,  305,\n",
       "          2217]),\n",
       "  False],\n",
       " [tensor([2216, 1312,   58, 1194,  536,  320, 2217,  424,  359,  341, 1790, 1935,\n",
       "          2217]),\n",
       "  False],\n",
       " [tensor([2216,  413,  272,  405,  841,  627, 2217,  119,  316,  104,  395,  276,\n",
       "          2217]),\n",
       "  False],\n",
       " [tensor([2216,  270,  400, 1111,  405,  441, 2217,  258,  316,   32, 2082,  370,\n",
       "          2217]),\n",
       "  True]]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b9fc7dcd-37d4-47db-8409-fd7f68152d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2216, 2218,  309,  651,  108, 2218,  260, 2218, 2217,  368,  391,  269,\n",
       "         2094,  773,  391,  269, 2218]),\n",
       " tensor([2216,   32,  309,  651,  108, 1410,  260,  330, 2217,  368,  391,  269,\n",
       "         2094,  773,  391,  269, 2218]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_BERT_batch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d130f9e-1a13-4f40-a627-f7949eea4e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2218"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c712c77-69eb-4d1a-8d66-8dae9ee7ddfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
