replicating makemore from karpathy [https://github.com/karpathy/makemore](https://github.com/karpathy/makemore)

this is the eval cross-entropy loss on words.txt dataset
- bigram: ~2.48
- MLP: ~2.36
- MLP tuned: ~2.19
- MLP normalized: ~2.11
- MLP normalized tuned: ~2.03

