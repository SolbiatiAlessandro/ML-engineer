import torch
import sys
import os
import tiktoken

from time import time
import os
import pickle
import gc

import torch
from dataclasses import dataclass

class ShardLoader:
    """not genereated by LLM -> code can be read"""
    
    def __init__(self, shard_filenames):
        self.END_OF_TEXT_TOKEN = 50256 
        
        self.shards = shard_filenames
        self.shard_ix = 0
        
        self.data_ix = 0
        self.data = self._get_shard_data(self.shard_ix)
        
    def _get_shard_data(self, shard_ix):
        shard_filename = self.shards[shard_ix]
        print(f"[ShardLoader._get_shard_data] loading {shard_filename}")
        t0 = time()
        with open(shard_filename, 'rb') as file:
            data = pickle.load(file) # [[token1, token2], [token1, token3]
        t1 = time()
        print(f"[ShardLoader._get_shard_data] {t1 - t0:.2f}s to load shard {shard_filename}")
        return data
    
    def _advance_data_ix(self):
        if self.data_ix == (len(self.data) - 1):
            self.shard_ix += 1
            if self.shard_ix == len(self.shards):
                self.shard_ix = 0
            self.data = self._get_shard_data(self.shard_ix)
            gc.collect()
            self.data_ix = 0
        else:
            self.data_ix += 1
        
    def get_next_tokens(self, n_tokens):
        """possible improvements
        - each time we start at 0 of next document, we disregard the
        last part of the document we haven't read last time"""
        tokens_left = n_tokens
        tokens = self.data[self.data_ix][:tokens_left]
        tokens_left -= len(tokens)
        self._advance_data_ix()
        while tokens_left > 0:
            tokens += [self.END_OF_TEXT_TOKEN]
            tokens_left -= 1
            new_tokens = self.data[self.data_ix][:tokens_left]
            tokens += new_tokens
            self._advance_data_ix()
            tokens_left -= len(new_tokens)
        return tokens
    

class DataLoader:
    def __init__(self, config, process_rank=0, num_processes=1):
        """not generated by LLM -> code be read"""
        
        self.config = config
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.batch_size = self.config.mini_batch_size
       
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.vocab_size = self.tokenizer.n_vocab
        
        print(f"[DataLoader.__init__] loaded tokenizer=tiktoken.{self.tokenizer.name}, vocab_size={self.vocab_size}")
        
        #shard_dir = "./tokenized_shards"
        shard_dir = config.dataset_directory
        shard_filenames = list(map(lambda x: shard_dir + "/" + x, sorted(os.listdir(shard_dir))))

        self.train_shard_loader =  ShardLoader(shard_filenames[:-1])
        # advance loader to correct GPU by discarding tokens
        self.train_shard_loader.get_next_tokens(self.batch_step * self.process_rank)
        
        self.val_shard_loader = ShardLoader(shard_filenames[-1:])
        
        
        
      
        
    @property
    def batch_step(self):
        """Dynamic getter for batch_step based on current batch size"""
        return self.batch_size  * self.config.block_size

    def next_batch(self, mode="train", device='cpu', batch_size=None, debug=True):
        """ mode=["train", "eval"] """
        if batch_size:
            self.batch_size = batch_size
        if mode == "train":
            shard_loader = self.train_shard_loader
        else:
            shard_loader = self.val_shard_loader
        
        if debug:
            t0 = time()
            print(f"[DataLoader.next_batch]"
                  f" process_rank={self.process_rank}"
                  f" shard_loader.data_ix={shard_loader.data_ix}"
                  f" shard_loader.shard_ix={shard_loader.shard_ix}")
        buf = torch.tensor(shard_loader.get_next_tokens(self.batch_step + 1))
        x = buf[:-1].view(self.batch_size, self.config.block_size)
        y = buf[1:].view(self.batch_size, self.config.block_size)
        
        # we discard these tokens
        shard_loader.get_next_tokens(self.batch_step * self.num_processes)
        
        if debug:
            t1 = time()
            print(f"[DataLoader.next_batch] batch_size={batch_size} completed in {t1 - t0}s")
    
        return x.to(device), y.to(device)
    
   
    
def test_shard_loader():
    import tiktoken
    enc = tiktoken.get_encoding("gpt2")

    shard_dir = "./tokenized_shards"
    shard_filenames = list(map(lambda x: shard_dir + "/" + x, sorted(os.listdir(shard_dir))))
    shard_loader = ShardLoader(shard_filenames)
    t = shard_loader.get_next_tokens(1000)
    assert len(t) == 1000
    print("[test_shard_loader] TEST 1 PASSED")
    t = shard_loader.get_next_tokens(10000)
    assert len(t) == 10000
    assert "<|endoftext|>" in enc.decode(t)
    print("[test_shard_loader] TEST 2 PASSED")
    shard_loader.data_ix = len(shard_loader.data) - 1
    t = shard_loader.get_next_tokens(10000)
    assert len(t) == 10000
    assert "<|endoftext|>" in enc.decode(t)
    print("[test_shard_loader] TEST 3 PASSED")
    
    shard_loader.shard_ix = len(shard_loader.shards) - 1
    shard_loader.data = shard_loader._get_shard_data(shard_loader.shard_ix)
    t = shard_loader.get_next_tokens(10000)
    assert len(t) == 10000
    assert "<|endoftext|>" in enc.decode(t)
    print("[test_shard_loader] TEST 4 PASSED")
    
    
@dataclass
class TestConfig:
    block_size: int = 1024
    vocab_size: int = 50257
    n_embd: int = 768
    batch_size: int = 1
    n_layer: int = 12
    n_head: int = 12
    
def test_data_loader():
    config = TestConfig
    config.mini_batch_size = 32
    config.total_batch_size = 64*64
    config.block_size = 1024
    config.epochs = 1000000
    config.validation_frequency = 10
    config.validation_epochs = 5
    config.tokenizer_name = "gpt2"
    config.dataset_directory = "./tokenized_shards"
    config.downstream_evals_iterations = 300
    config.downstream_evals_frequency = 100

    dataloader = DataLoader(config)
    print("[test_data_loader] TEST PASSED (loading)")
    
    batch_size = 1000
    x, y  = dataloader.next_batch(batch_size=batch_size)
    x.shape == torch.Size([batch_size, config.block_size])
    y.shape == torch.Size([batch_size, config.block_size])
    print("[test_data_loader] TEST PASSED (small batch)")
    
    batch_size = 300000
    x, y  = dataloader.next_batch(batch_size=batch_size)
    x.shape == torch.Size([batch_size, config.block_size])
    y.shape == torch.Size([batch_size, config.block_size])
    print("[test_data_loader] TEST PASSED (cross shard)")
    
    
    
if __name__ == "__main__":
    test_shard_loader()
    test_data_loader()