import torch
import sys
import os
import tiktoken

from time import time
import os
import pickle
import gc

class ShardLoader:
    """not genereated by LLM -> code can be read"""
    
    def __init__(self, shard_dir="./tokenized_shards/"):
        self.shard_dir = shard_dir
        self.END_OF_TEXT_TOKEN = 50256
        
        self.shards = sorted(os.listdir("./tokenized_shards"))
        self.shard_ix = 0
        
        self.data_ix = 0
        self.data = self._get_shard_data(self.shard_ix)
        
    def _get_shard_data(self, shard_ix):
        shard_filename = self.shard_dir + self.shards[shard_ix]
        print(f"[ShardLoader] loading {shard_filename}")
        t0 = time()
        with open(shard_filename, 'rb') as file:
            data = pickle.load(file) # [[token1, token2], [token1, token3]
        t1 = time()
        print(f"[ShardLoader] {t1 - t0:.2f}s to load shard {shard_filename}")
        return data
    
    def _advance_data_ix(self):
        if self.data_ix == (len(self.data) - 1):
            self.shard_ix += 1
            if self.shard_ix == len(self.shards):
                self.shard_ix = 0
            self.data = self._get_shard_data(self.shard_ix)
            gc.collect()
            self.data_ix = 0
        else:
            self.data_ix += 1
        
    def get_next_tokens(self, n_tokens):
        tokens_left = n_tokens
        tokens = self.data[self.data_ix][:tokens_left]
        tokens_left -= len(tokens)
        self._advance_data_ix()
        while tokens_left > 0:
            tokens += [self.END_OF_TEXT_TOKEN]
            tokens_left -= 1
            new_tokens = self.data[self.data_ix][:tokens_left]
            tokens += new_tokens
            self._advance_data_ix()
            tokens_left -= len(new_tokens)
        return tokens
    
def test_shard_loader():
    shard_loader = ShardLoader()
    t = shard_loader.get_next_tokens(1000)
    assert len(t) == 1000
    print("[test_shard_loader] TEST 1 PASSED")
    t = shard_loader.get_next_tokens(10000)
    assert len(t) == 10000
    assert "<|endoftext|>" in enc.decode(t)
    print("[test_shard_loader] TEST 2 PASSED")
    shard_loader.data_ix = len(shard_loader.data) - 1
    t = shard_loader.get_next_tokens(10000)
    assert len(t) == 10000
    assert "<|endoftext|>" in enc.decode(t)
    print("[test_shard_loader] TEST 3 PASSED")
    
    shard_loader.shard_ix = len(shard_loader.shards) - 1
    shard_loader.data = shard_loader._get_shard_data(shard_loader.shard_ix)
    t = shard_loader.get_next_tokens(10000)
    assert len(t) == 10000
    assert "<|endoftext|>" in enc.decode(t)
    print("[test_shard_loader] TEST 4 PASSED")



class DataLoader:
    def __init__(self, config, process_rank=0, num_processes=1):
        """not generated by LLM -> code be read"""
        
        self.config = config
        self.process_rank = process_rank
        self.num_processes = num_processes
        dataset_name = config.dataset
        self.DATASET_FOLDER = "./datasets"
       
        self.tokenizer = tiktoken.get_encoding("gpt2")
        self.vocab_size = self.tokenizer.n_vocab
        
        import os
        self.shards = sorted(os.listdir("./tokenized_shards"))
        self.shard_ix = 0
        
        print(f"[DataLoader.__init__] loaded tokenizer=tiktoken.{self.tokenizer.name}, vocab_size={self.vocab_size}")

        self.train_data =  self._get_shard_data(self.shard_ix)
        self.val_data = self._get_shard_data(len(self.shards) - 1)
        
        
        self.batch_size = self.config.batch_size
        self.train_data_ix = self.batch_size * self.process_rank
        self.val_data_ix = self.batch_size * self.process_rank
        
    def _get_shard_data(self, shard_ix):
        shard_filename = self.shards[shard_ix]
        with open("./tokenized_shards/" + shard_filename, 'rb') as file:
            return pickle.load(file) # [[token1, token2], [token1, token3]]
        
    @property
    def batch_step(self):
        """Dynamic getter for batch_step based on current batch size"""
        return self.batch_size  * self.config.block_size
        
    def _load_dataset(self, filename):
        if self.tokenizer is None: 
            raise Exception("[DataloaderException] Need to load dataset after loading tokenizer")
        filepath = os.path.join(self.DATASET_FOLDER, filename)
        cache_file = filepath + "_" + self.tokenizer.name + ".cache.pt"  # cache file path

        if os.path.exists(cache_file):
            print(f"[DataLoader._load_dataset] Loading cached encoding from {cache_file}")
            encoded_dataset = torch.load(cache_file)
            
        else:
            with open(filepath, 'r') as f:
                text = f.read()
            print(f"[DataLoader._load_dataset] {filepath}: size = {len(text)}")
            encoded_dataset = self.tokenizer.encode(text)
            print(f"[DataLoader._load_dataset] {filepath}: max vocabulary size = {max(encoded_dataset)}, compression ratio = {len(encoded_dataset) / len(text)}")
            # Save the computed encoding to cache
            torch.save(encoded_dataset, cache_file)
            print(f"[DataLoader._load_dataset] Saved cached encoding to {cache_file}")
            
        return torch.tensor(encoded_dataset, device='cpu')

    def next_batch(self, mode="train", device='cpu', batch_size=None):
        """ mode=["train", "eval"] """
        if batch_size:
            self.batch_size = batch_size
        if mode == "train":
            x, y = self._next_batch_train()
        else:
            x, y = self._next_batch_eval()
        return x.to(device), y.to(device)
    
    def _next_batch_train(self):
        
        data = self.train_data
        ix = self.train_data_ix 
        
        # question for karpathy, how do we handle document change?
        # either put thing in the middle or have only
        # text from one document in the batch (better)
        buf = data[ix:ix+self.batch_step + 1]     
        x = buf[:-1].view(self.batch_size, self.config.block_size)
        y = buf[1:].view(self.batch_size, self.config.block_size)
        
        self.train_data_ix += self.batch_step * self.num_processes
        if self.train_data_ix + (self.batch_step * self.num_processes) + 1 > len(self.train_data):
            self.train_data_ix = self.batch_size * self.process_rank
        
        return x, y
    
    def _next_batch_eval(self):
        
        data = self.val_data
        ix = self.val_data_ix 
        
        buf = data[ix:ix+self.batch_step + 1]     
        x = buf[:-1].view(self.batch_size, self.config.block_size)
        y = buf[1:].view(self.batch_size, self.config.block_size)
        
        self.val_data_ix += self.batch_step * self.num_processes
        if self.val_data_ix + (self.batch_step * self.num_processes) + 1 > len(self.val_data):
            self.val_data_ix = self.batch_size * self.process_rank
        
        return x, y
    

        
        

