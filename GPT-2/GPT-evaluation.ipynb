{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590a83b-1661-4bf7-8fc0-d55731083644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_epoch_40000_GPT_vocab18k > Final accuracy on validation: 34.91% (74/212)\n",
    "# checkpoint_epoch_5000_GPT_vocab18k  > Final accuracy on validation: 18.40% (39/212)\n",
    "#                                     > Final accuracy on validation: 15.15% (20/132)\n",
    "# checkpoint_epoch_1000_GPT_vocab18k  > Final accuracy on validation: 8.49% (18/212)\n",
    "\n",
    "\n",
    "# checkpoint_epoch_5000_GPT > Final accuracy on validation: 46.21% (61/132)\n",
    "# checkpoint_epoch_1000_GPT > Final accuracy on validation: 12.12% (16/132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180294e4-84e6-4cc7-8edb-5a0532971390",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "769a7eba-4647-4add-9ca0-765e9fc35ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e02ca6eb-83e1-482e-9035-255ed3251c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from GPT import GPT, GPTConfig\n",
    "from dataloader import DataLoader\n",
    "\n",
    "config = GPTConfig()\n",
    "config.batch_size = 28\n",
    "config.block_size = 1024\n",
    "config.epochs = 1000000\n",
    "config.validation_frequency = 100\n",
    "config.validation_epochs = 5\n",
    "config.dataset = \"wikitext\"\n",
    "config.tokenizer_name = \"wikitext2_18k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "68006f12-ad3b-431a-a2c4-d8814b368dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer.__init__] 0 regex tokens (order preserved)\n",
      "[DataLoader.__init__] loaded tokenizer tokenizer_wikitext2_18k.pickle\n",
      "[DataLoader._load_dataset] Loading cached encoding from ./datasets/wikitext_train.txt.cache.pt\n",
      "[DataLoader._load_dataset] ./datasets/wikitext_train.txt: vocab_size = 17707\n",
      "[DataLoader._load_dataset] Loading cached encoding from ./datasets/wikitext_val.txt.cache.pt\n",
      "[DataLoader._load_dataset] ./datasets/wikitext_val.txt: vocab_size = 17707\n",
      "[DataLoader.__init__] train_data.shape=torch.Size([208821909]), val_data.shape=torch.Size([431117])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(config)\n",
    "config.vocab_size = data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "69d68535-73c3-40a7-b33a-4b32fa269471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17707"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9c9a7555-1dbd-48fc-a6cd-2995f2065200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99,441,408\n"
     ]
    }
   ],
   "source": [
    "model = GPT(config)\n",
    "model.to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3ae5444e-c8cd-4341-bd67-6bd5e52e390d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from ./checkpoints/checkpoint_epoch_40000_GPT_vocab18k.pth, resuming at epoch 40000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40000, 'GPT_vocab18k')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_checkpoint\n",
    "checkpoint_path = \"./checkpoints/checkpoint_epoch_40000_GPT_vocab18k.pth\"\n",
    "load_checkpoint(model, None, checkpoint_path, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "524022c5-3fcf-4312-8e41-df3251493d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from generate import generate, sample_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a2c3357c-a388-468b-bb29-04d9ce60951d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate] generated: When he went to the American public to criticize the album . After a breakup in the United Sta\n",
      "[generate] generated: After the other two were to be abandoned by the influenza system of HMS Warspite ( see the\n",
      "[generate] generated: Hello world @-@ record holder in the professional role , the team played in a\n",
      "[generate] generated: Often it was assumed this informal title and wailst to resolve the revolt . \n",
      " The\n",
      "[generate] generated: 2+2= , and everyone can never be disturbed by such an extensive observations\n",
      "[generate] generated: The capital of France is xenon @-@ era shared with the social divisions that are allocated to the conso\n",
      "[generate] generated: The pen is on the xylomand spacecraft . \n",
      " = = Reception = = \n",
      " = = = Spectral r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('When he went',\n",
       "  'When he went to the American public to criticize the album . After a breakup in the United Sta'),\n",
       " ('After the other',\n",
       "  'After the other two were to be abandoned by the influenza system of HMS Warspite ( see the'),\n",
       " ('Hello world',\n",
       "  'Hello world @-@ record holder in the professional role , the team played in a'),\n",
       " ('Often it was',\n",
       "  'Often it was assumed this informal title and wailst to resolve the revolt . \\n The'),\n",
       " ('2+2=',\n",
       "  '2+2= , and everyone can never be disturbed by such an extensive observations'),\n",
       " ('The capital of France is ',\n",
       "  'The capital of France is xenon @-@ era shared with the social divisions that are allocated to the conso'),\n",
       " ('The pen is on the ',\n",
       "  'The pen is on the xylomand spacecraft . \\n = = Reception = = \\n = = = Spectral r')]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_generations(model, data_loader.tokenizer, \n",
    "    config, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e12be807-3a86-408b-90df-2b6a670d5841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate] generated: When he went to the American public to criticize the album . After a breakup in the United States , Texas headed for MuchMusic 's Rock Band . \n",
      " = = = = = Both AC / DC and the AC / DC Foundation remix = = = = = \n",
      " After the release of Thursday Nightmares\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"When he went to the American public to criticize the album . After a breakup in the United States , Texas headed for MuchMusic 's Rock Band . \\n = = = = = Both AC / DC and the AC / DC Foundation remix = = = = = \\n After the release of Thursday Nightmares\""
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model, \n",
    "    data_loader.tokenizer, \n",
    "    config, \n",
    "    device=device,\n",
    "    prompt=\"When he went\",\n",
    "    length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "99f4618d-f5cb-4331-aa5c-8149669b2a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   73,   110,  1984,    44,   300,  1782,   429,  8685,   110,   334,\n",
       "         13560,   108,   268,  5108,   630,   100,   268,   630,   280,   260,\n",
       "            32]], device='cuda:0')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"In 2007, American Idol opened one of the \"\"\"\n",
    "tokens = data_loader.tokenizer.encode(prompt)\n",
    "tokens\n",
    "model.eval()\n",
    "x = torch.tensor(tokens[:config.block_size], device=device).view(1, -1)\n",
    "x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2dc4aa46-b283-4e23-ae33-36d417b2f17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 2007, American Idol opened one of the '"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.tokenizer.decode(tokens, raw_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "bc00dd77-c5a8-47d0-83dc-59b2b7433afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2007, American Idol opened one of the # 10 American Stop , 2009 . Around 1 @,@ 000 of a philosophy of the philosophical launched the philosophical launch of the stop of the philosophy 's stop for the philosophy stop . \n",
      " This features in the level of the game 's development for instance . The contrast over the game 's role , a new philosophical stop philosophical exist in the stop of regular philosophy . It was established as stop paint prisoners and a contrast to unit for the\n"
     ]
    }
   ],
   "source": [
    "to_decode = x.tolist()[0]\n",
    "for _ in range(200):\n",
    "    \n",
    "    x = x[-config.block_size:]\n",
    "\n",
    "    logits = model(x.view(1, -1))\n",
    "    \n",
    "    v, ixs = logits[0, -1, :].topk(20)\n",
    "    ix = torch.multinomial(F.softmax(v, dim=0), 1)\n",
    "    new_token = ixs[ix]\n",
    "    to_decode.append(new_token.view(-1).item())\n",
    "    x = torch.cat([x.view(-1), new_token])\n",
    "\n",
    "print(data_loader.tokenizer.decode(to_decode, raw_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9dc1c-8283-4deb-a1b3-055b47b64711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea0703-cec7-47c3-babc-fa860fb60bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ab3d2-9071-4a09-a828-a064662a5dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "04e482fe-3ab3-4d80-971e-5ee15b9f0012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from evaluation import evaluate_cbt_with_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ff60861f-57b3-4142-b471-e34cd56b3cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on CBT-CN validation set...\n",
      "Max context length: 800 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation examples: 100%|██████████| 300/300 [00:20<00:00, 14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final accuracy on validation: 15.15% (20/132)\n",
      "Skipped examples due to context length: 168\n",
      "\n",
      "Top-K Accuracy:\n",
      "Top-1 accuracy: 15.15%\n",
      "Top-2 accuracy: 31.06%\n",
      "Top-3 accuracy: 46.97%\n",
      "Top-4 accuracy: 56.82%\n",
      "Top-5 accuracy: 69.70%\n",
      "\n",
      "Token Count Statistics:\n",
      "Min: 436\n",
      "Max: 799\n",
      "Mean: 630.3\n",
      "Median: 639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, results, skipped = evaluate_cbt_with_probs(\n",
    "        model=model,\n",
    "        tokenizer=data_loader.tokenizer,\n",
    "        device=device,\n",
    "        dataset_split=\"validation\",\n",
    "        verbose=False,\n",
    "        max_context_length=800,\n",
    "        max_examples=300  # Set to None to evaluate all examples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "44c38a68-3088-49a3-a755-44ebe66eb5bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15151515151515152, 168)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9ff37b4e-6fa2-40f3-8aba-aa1746ccfe64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1707, 100, 287, 100]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.tokenizer.encode(\"ASdasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5c0d3-2ed8-4bd4-a73b-f9b15f2024a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834464b-617b-4e71-9d05-6ba66042ae78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ccf82-174f-49d1-b628-29d4acdcaa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_checkpoint\n",
    "checkpoint_path = \"./checkpoints/checkpoint_epoch_40000_GPT_vocab18k.pth\"\n",
    "load_checkpoint(model, None, checkpoint_path, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5548de1-2081-4af9-8bf1-4e8b98c2b765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gpt2-large on CBT-CN validation set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dataset_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m accuracy, results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_cbt_with_probs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to None to evaluate all examples\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/notebooks/ML-system-design/GPT-2/evaluation.py:79\u001b[0m, in \u001b[0;36mevaluate_cbt_with_probs\u001b[0;34m(model_name, dataset_split, verbose, max_examples)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[1;32m     78\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 79\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Move to GPU if available\u001b[39;00m\n\u001b[1;32m     82\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3236\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3236\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:949\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 949\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mn_embd, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:675\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39membd_pdrop)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[43m[\u001b[49m\u001b[43mGPT2Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:675\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39membd_pdrop)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mGPT2Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:375\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrossattention \u001b[38;5;241m=\u001b[39m GPT2Attention(config, is_cross_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_cross_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2MLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py:349\u001b[0m, in \u001b[0;36mGPT2MLP.__init__\u001b[0;34m(self, intermediate_size, config)\u001b[0m\n\u001b[1;32m    347\u001b[0m embed_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc \u001b[38;5;241m=\u001b[39m Conv1D(intermediate_size, embed_dim)\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj \u001b[38;5;241m=\u001b[39m \u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mactivation_function]\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39mresid_pdrop)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[0;34m(self, nf, nx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(nx, nf))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(nf))\n\u001b[0;32m--> 103\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Choose model size - larger models will be more accurate but require more GPU memory\n",
    "# Options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "model_name = \"gpt2-large\"\n",
    "\n",
    "# Choose dataset split\n",
    "dataset_split = \"validation\"\n",
    "\n",
    "# Run evaluation\n",
    "accuracy, results = evaluate_cbt_with_probs(\n",
    "    model_name=model_name,\n",
    "    dataset_split=dataset_split,\n",
    "    verbose=True,\n",
    "    max_examples=50  # Set to None to evaluate all examples\n",
    ")\n",
    "\n",
    "print(f\"\\nEvaluation complete!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Dataset: CBT-CN {dataset_split}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566bd56-8e3a-4171-b594-0e68410d568c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
