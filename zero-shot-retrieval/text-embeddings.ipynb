{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":35704,"databundleVersionId":3515347,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alessandrosolbiati/zero-shot-learning-vmware-text-embedding?scriptVersionId=222081016\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:20:44.446382Z","iopub.execute_input":"2025-02-12T01:20:44.446725Z","iopub.status.idle":"2025-02-12T01:20:44.765025Z","shell.execute_reply.started":"2025-02-12T01:20:44.446696Z","shell.execute_reply":"2025-02-12T01:20:44.764083Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vmware-zero-shot-information-retrieval/sample_submission.csv\n/kaggle/input/vmware-zero-shot-information-retrieval/vmware_ir_content.csv\n/kaggle/input/vmware-zero-shot-information-retrieval/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"hyperparameters = {\n    'chunk_size_characters': 400,\n    'chunk_size_padding': 50,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:20:44.829513Z","iopub.execute_input":"2025-02-12T01:20:44.829881Z","iopub.status.idle":"2025-02-12T01:20:44.834097Z","shell.execute_reply.started":"2025-02-12T01:20:44.829856Z","shell.execute_reply":"2025-02-12T01:20:44.83285Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Chunk Documents","metadata":{"execution":{"iopub.status.busy":"2025-02-11T01:14:53.820782Z","iopub.execute_input":"2025-02-11T01:14:53.821095Z","iopub.status.idle":"2025-02-11T01:14:53.82444Z","shell.execute_reply.started":"2025-02-11T01:14:53.821075Z","shell.execute_reply":"2025-02-11T01:14:53.823436Z"}}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/test.csv\")\n\n#sorted(list(test.Query), key=len)[1000:1100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:46:32.811043Z","iopub.execute_input":"2025-02-12T01:46:32.81136Z","iopub.status.idle":"2025-02-12T01:46:32.821126Z","shell.execute_reply.started":"2025-02-12T01:46:32.811335Z","shell.execute_reply":"2025-02-12T01:46:32.820374Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"pp = filter(lambda x: 'network' in x, list(test.Query))\nnetwork_queries = list(pp)\nnetwork_queries","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:47:29.227648Z","iopub.execute_input":"2025-02-12T01:47:29.228014Z","iopub.status.idle":"2025-02-12T01:47:29.235113Z","shell.execute_reply.started":"2025-02-12T01:47:29.227958Z","shell.execute_reply":"2025-02-12T01:47:29.234354Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"['what is network virtualization',\n 'what is cloud networking',\n 'what is virtual network',\n 'what is a virtual network',\n 'what is network configuration',\n 'what is network access control',\n 'postgresql replication is not in progress. verify if postgresql server is running on the passive node and that the passive node is reachable on the vcenter ha network.',\n 'what is network automation',\n 'failed to get management network information. verify if management interface (nic0) is configured correctly and is reachable, and verify if correct dns mapping is provided for forward and reverse hostname lookup.',\n 'this virtual machine might have been moved or copied. in order to configure certain management and networking features, vmware esx needs to know if this virtual machine was moved or copied. if you don\\'t know, answer \"i copied it\".',\n 'what is intent based networking',\n 'what is the core component of intent-based networking?',\n 'what is enterprise network',\n 'how to configure a nat network in vmware workstation',\n 'what is storage area network',\n 'how do servers connect to the network in a virtual environment',\n 'what is virtualization in networking',\n 'what is an enterprise network',\n 'what is virtual networking',\n 'what is network convergence',\n 'what is cloud network',\n 'what is converged network',\n 'what is convergence in networking',\n 'what is software defined networking',\n 'what is enterprise networking',\n 'the vsphere ha agent on this host cannot reach some of the management network addresses of other hosts, and ha may not be able to restart vms if a host failure occurs:',\n 'what is a cloud network',\n 'what is a converged network',\n 'what is intent-based networking',\n 'the feature you are trying to use is on a network resource that is unavailable when installing',\n 'what is micro segmentation in networking',\n 'what is the purpose of having a converged network?',\n 'what is container networking',\n 'how does software defined networking work',\n 'what is network infrastructure security',\n 'what is cloud in computer network',\n 'what is cloud in networking',\n 'what is network virtualization in cloud computing',\n 'what is san in networking',\n 'what is the core component of internet-based networking',\n 'what is network security',\n 'how storage area network works',\n 'what is a federated network',\n 'what are virtual networks',\n 'what is network infrastructure',\n 'how to disable vmware network adapter windows 10',\n 'the vsphere ha agent on this host cannot reach some of the management network addresses of other hosts, and ha may not be able to restart vms if a host failure occurs',\n 'how does network access control work',\n 'how to connect virtual machine to physical network',\n 'how possible are the traditional separate data, telephone, and video networks be converged together into one platform?',\n 'what is a network cloud',\n 'how to remove vmware network adapter',\n 'what is the core component of intent-based networking',\n 'what is kubernetes networking',\n 'what is a benefit of using vsphere distributed switches in a vsan network?',\n 'what is the maximum number of hosts per vnetwork distributed switch?',\n 'what is configuration in networking',\n 'what is a san in networking',\n 'what is meant by a virtual network',\n 'what is the purpose of having a converged network']"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"import pandas as pd\ncontent = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/vmware_ir_content.csv\")\nlen(content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:20:55.882679Z","iopub.execute_input":"2025-02-12T01:20:55.883031Z","iopub.status.idle":"2025-02-12T01:21:07.185242Z","shell.execute_reply.started":"2025-02-12T01:20:55.883005Z","shell.execute_reply":"2025-02-12T01:21:07.18448Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"323963"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"content = content[content.raw_text.fillna(\"\").apply(lambda x: 'network' in x)]\nlen(content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:07.186203Z","iopub.execute_input":"2025-02-12T01:21:07.186506Z","iopub.status.idle":"2025-02-12T01:21:07.952822Z","shell.execute_reply.started":"2025-02-12T01:21:07.186481Z","shell.execute_reply":"2025-02-12T01:21:07.952062Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"62371"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"content['raw_text'].str.len().sum() # 750M\n# num of chunks 750M / 20 = 30M ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:07.956743Z","iopub.execute_input":"2025-02-12T01:21:07.956993Z","iopub.status.idle":"2025-02-12T01:21:08.017166Z","shell.execute_reply.started":"2025-02-12T01:21:07.95696Z","shell.execute_reply":"2025-02-12T01:21:08.016469Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"275640277"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"sampled_content = content.sample(frac=0.01)\nlen(sampled_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:08.018165Z","iopub.execute_input":"2025-02-12T01:21:08.018492Z","iopub.status.idle":"2025-02-12T01:21:08.025738Z","shell.execute_reply.started":"2025-02-12T01:21:08.018469Z","shell.execute_reply":"2025-02-12T01:21:08.024953Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"624"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from dataclasses import dataclass\n\nchunk_size = hyperparameters['chunk_size_characters']\npadding = hyperparameters['chunk_size_padding']\nCHUNK_MAX_LENGTH = chunk_size + 2 * padding\n\ndef chunkify(document):\n    ix, chunks = 0, []\n    while ix < len(document):\n        chunk_start = ix \n        chunk_end   = ix + padding + chunk_size + padding\n        chunks.append(document[chunk_start:chunk_end])\n        ix = ix + padding + chunk_size\n    return chunks\n\n@dataclass\nclass Chunk:\n    document_id: int\n    chunk_data: str\n\nchunks = [] # id: chunk\nfor ix, (k,v) in enumerate(list(content['raw_text'].fillna(\"\").items())):\n\n    kchunks = chunkify(v)\n    for kchunk in kchunks:\n        if \"network\" in kchunk:\n            chunks.append(Chunk(document_id=k, chunk_data=kchunk))\n    if ix % 10000 == 0:\n        print(f\"id: {k}, passage length: {len(v)}, number of chunks: {len(kchunks)}\")\nprint(len(chunks))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:08.026543Z","iopub.execute_input":"2025-02-12T01:21:08.026844Z","iopub.status.idle":"2025-02-12T01:21:09.106555Z","shell.execute_reply.started":"2025-02-12T01:21:08.026815Z","shell.execute_reply":"2025-02-12T01:21:09.10549Z"}},"outputs":[{"name":"stdout","text":"id: 3, passage length: 3918, number of chunks: 9\nid: 49212, passage length: 1915, number of chunks: 5\nid: 104920, passage length: 1252, number of chunks: 3\nid: 157021, passage length: 3230, number of chunks: 8\nid: 208526, passage length: 3191, number of chunks: 8\nid: 259705, passage length: 1762, number of chunks: 4\nid: 311489, passage length: 4123, number of chunks: 10\n151101\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"c1 = chunks[0].chunk_data\nc2 = chunks[30].chunk_data\nc1, c2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Embedding model","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:12.592939Z","iopub.execute_input":"2025-02-12T01:21:12.593329Z","iopub.status.idle":"2025-02-12T01:21:14.192915Z","shell.execute_reply.started":"2025-02-12T01:21:12.593302Z","shell.execute_reply":"2025-02-12T01:21:14.191883Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = [f'query: {chunks[0].chunk_data}',\n               f'query: {chunks[-10].chunk_data}',\n               f'query: {chunks[1].chunk_data}',\n               f'query: {chunks[-11].chunk_data}']\ninput_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-small-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-small-v2').to(device)\n#  model alone on GPU T4 is 243MiB /  15360MiB \n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\ndef model_predict(input_texts: list[str], model=model, tokenizer=tokenizer):\n    # Tokenize the input texts\n    batch_dict = tokenizer(input_texts, max_length=CHUNK_MAX_LENGTH, padding=True, truncation=True, return_tensors='pt')\n    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n    with torch.no_grad():\n        outputs = model(**batch_dict)\n        embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n        # normalize embeddings\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n    return embeddings, batch_dict\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:50.902043Z","iopub.execute_input":"2025-02-12T01:21:50.902627Z","iopub.status.idle":"2025-02-12T01:21:56.380104Z","shell.execute_reply.started":"2025-02-12T01:21:50.902597Z","shell.execute_reply":"2025-02-12T01:21:56.379354Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nembeddings, _ = model_predict(input_texts)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunked_corpus = [f\"query: {chunk.chunk_data}\" for chunk in chunks][:3000]\nlen(chunked_corpus)\n\n# 1315MiB /  15360MiB after first inference of 300\n# 2709MiB /  15360MiB inference of 600\n# 13913MiB /  15360MiB  inference for 3000 chunks, takes 3 seconds (3 * 10^3 seconds)\n\n# total number of chunks is 3*10^7 = 10^4 seconds to embed them all = 3 hours\n# I have two GPUs and I can do it in parallel and is going to take 1 hour\n\n# 3000 embeddings are 3000 * 348 emb size * 4bytes = 1MB in memory  = 10^6\n# I have 10GB memory 10^10 , can hold 3 * 10^3 * 10^4 = 3 10^7 embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum([len(c) for c in chunked_corpus])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from time import time\nstart_time = time()\nembeddings = model_predict(chunked_corpus)\nend_time = time()\nprint(f\"Model inference for {len(chunked_corpus)} chunks: {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(embeddings.shape)\nend_time = time()\nprint(f\"Model inference for {len(chunked_corpus)} chunks: {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:48:59.805542Z","iopub.execute_input":"2025-02-12T01:48:59.805852Z","iopub.status.idle":"2025-02-12T01:49:00.097195Z","shell.execute_reply.started":"2025-02-12T01:48:59.805829Z","shell.execute_reply":"2025-02-12T01:49:00.096119Z"}},"outputs":[{"name":"stdout","text":"Wed Feb 12 01:48:59 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   62C    P0             30W /   70W |     479MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   36C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass ChunkTextDataset(Dataset):\n    def __init__(self, chunks: list[Chunk]):\n        # Prepend \"query: \" to each chunk's text.\n        self.texts = [f\"query: {chunk.chunk_data}\" for chunk in chunks]\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return self.texts[idx]\n\n\ndataset = ChunkTextDataset(chunks)\ndataloader = DataLoader(dataset, batch_size=1000, shuffle=False)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:22:07.303625Z","iopub.execute_input":"2025-02-12T01:22:07.304301Z","iopub.status.idle":"2025-02-12T01:22:07.378155Z","shell.execute_reply.started":"2025-02-12T01:22:07.304265Z","shell.execute_reply":"2025-02-12T01:22:07.377427Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:22:22.966496Z","iopub.execute_input":"2025-02-12T01:22:22.966803Z","iopub.status.idle":"2025-02-12T01:22:22.970825Z","shell.execute_reply.started":"2025-02-12T01:22:22.966781Z","shell.execute_reply":"2025-02-12T01:22:22.969872Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"all_embeddings = []\nimport gc\n    \n# Disable gradients for inference.\nwith torch.no_grad():\n    for batch in dataloader:\n\n        from time import time\n        start_time = time()\n\n        # Each 'batch' is a list of strings (already with \"query: \" prepended).\n        embeddings, batch_dict = model_predict(batch)  # Assume this returns a tensor of shape [batch_size, embedding_dim]\n\n        end_time = time()\n        \n        torch.cuda.empty_cache()\n        print(f\"Processed batch of {len(batch)} chunks, {end_time - start_time:.2f} seconds\")\n\n        # Move embeddings to CPU (if they are on GPU) and append.\n        all_embeddings.append(embeddings.cpu())\n\n        # Clean up GPU memory:\n        del batch_dict\n        del embeddings\n        del batch\n        torch.cuda.empty_cache()\n        gc.collect()\n              \n        \n\n        \n        \n# Concatenate all the batch embeddings into a single tensor.\nall_embeddings = torch.cat(all_embeddings, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:23:02.962629Z","iopub.execute_input":"2025-02-12T01:23:02.962938Z","iopub.status.idle":"2025-02-12T01:36:56.297856Z","shell.execute_reply.started":"2025-02-12T01:23:02.962915Z","shell.execute_reply":"2025-02-12T01:36:56.296759Z"}},"outputs":[{"name":"stdout","text":"Processed batch of 1000 chunks, 0.24 seconds\nProcessed batch of 1000 chunks, 0.24 seconds\nProcessed batch of 1000 chunks, 0.24 seconds\nProcessed batch of 1000 chunks, 0.23 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.27 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.38 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.40 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.37 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.38 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.38 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.26 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 101 chunks, 0.29 seconds\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"len(chunks)\n\nimport pickle\n\n\n\n# Save the chunks list to disk\nwith open('/kaggle/working/chunks151101_11_Feb_A__(network).pkl', 'wb') as f:\n    pickle.dump(chunks, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:40:55.360727Z","iopub.execute_input":"2025-02-12T01:40:55.361159Z","iopub.status.idle":"2025-02-12T01:40:55.73333Z","shell.execute_reply.started":"2025-02-12T01:40:55.361125Z","shell.execute_reply":"2025-02-12T01:40:55.732257Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"all_embeddings.shape\n\nnp.save('/kaggle/working/embeddings151101x384_11_Feb_A__(network).npy', all_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:39:34.939016Z","iopub.execute_input":"2025-02-12T01:39:34.939358Z","iopub.status.idle":"2025-02-12T01:39:35.09646Z","shell.execute_reply.started":"2025-02-12T01:39:34.939329Z","shell.execute_reply":"2025-02-12T01:39:35.095717Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"embeddings = all_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:43:33.921659Z","iopub.execute_input":"2025-02-12T01:43:33.922049Z","iopub.status.idle":"2025-02-12T01:43:33.926108Z","shell.execute_reply.started":"2025-02-12T01:43:33.922016Z","shell.execute_reply":"2025-02-12T01:43:33.9252Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Indexing","metadata":{"execution":{"iopub.status.busy":"2025-02-12T01:42:42.270647Z","iopub.execute_input":"2025-02-12T01:42:42.27108Z","iopub.status.idle":"2025-02-12T01:42:42.274821Z","shell.execute_reply.started":"2025-02-12T01:42:42.271046Z","shell.execute_reply":"2025-02-12T01:42:42.274068Z"}}},{"cell_type":"code","source":"!pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:43:51.476609Z","iopub.execute_input":"2025-02-12T01:43:51.477004Z","iopub.status.idle":"2025-02-12T01:43:58.456763Z","shell.execute_reply.started":"2025-02-12T01:43:51.476955Z","shell.execute_reply":"2025-02-12T01:43:58.455782Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"embeddings.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:44:08.376848Z","iopub.execute_input":"2025-02-12T01:44:08.377255Z","iopub.status.idle":"2025-02-12T01:44:08.382963Z","shell.execute_reply.started":"2025-02-12T01:44:08.377211Z","shell.execute_reply":"2025-02-12T01:44:08.38216Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"torch.Size([151101, 384])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import faiss\n\n# Get the dimensionality of your embeddings.\n\n\nembeddings_np = embeddings.cpu().detach().numpy().astype('float32')\n\ndimension = embeddings.shape[1]\n\n# Create a FAISS index. For example, use IndexFlatL2 for exact L2 (Euclidean) distance search:\nindex = faiss.IndexFlatL2(dimension)\n# For inner product similarity, you could use:\n# index = faiss.IndexFlatIP(dimension)\n\n# Add your embeddings to the index.\nindex.add(embeddings_np)\nprint(f\"Number of embeddings indexed: {index.ntotal}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:45:46.462691Z","iopub.execute_input":"2025-02-12T01:45:46.463172Z","iopub.status.idle":"2025-02-12T01:45:46.780574Z","shell.execute_reply.started":"2025-02-12T01:45:46.463132Z","shell.execute_reply":"2025-02-12T01:45:46.779528Z"}},"outputs":[{"name":"stdout","text":"Number of embeddings indexed: 151101\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"query_embeddings, _ = model_predict([f\"query: {query}\" for query in network_queries])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:49:06.885401Z","iopub.execute_input":"2025-02-12T01:49:06.885752Z","iopub.status.idle":"2025-02-12T01:49:06.910146Z","shell.execute_reply.started":"2025-02-12T01:49:06.88572Z","shell.execute_reply":"2025-02-12T01:49:06.909428Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"query_embeddings.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:49:09.339591Z","iopub.execute_input":"2025-02-12T01:49:09.339963Z","iopub.status.idle":"2025-02-12T01:49:09.345321Z","shell.execute_reply.started":"2025-02-12T01:49:09.339929Z","shell.execute_reply":"2025-02-12T01:49:09.344475Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"torch.Size([60, 384])"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"query_np = query_embeddings.cpu().detach().numpy().astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:49:35.010216Z","iopub.execute_input":"2025-02-12T01:49:35.010574Z","iopub.status.idle":"2025-02-12T01:49:35.015151Z","shell.execute_reply.started":"2025-02-12T01:49:35.010545Z","shell.execute_reply":"2025-02-12T01:49:35.014382Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"query_np.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:49:41.406519Z","iopub.execute_input":"2025-02-12T01:49:41.406838Z","iopub.status.idle":"2025-02-12T01:49:41.412085Z","shell.execute_reply.started":"2025-02-12T01:49:41.406815Z","shell.execute_reply":"2025-02-12T01:49:41.411106Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"(60, 384)"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"distances, indexes = index.search(query_np, 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:50:19.815266Z","iopub.execute_input":"2025-02-12T01:50:19.815634Z","iopub.status.idle":"2025-02-12T01:50:19.913499Z","shell.execute_reply.started":"2025-02-12T01:50:19.815606Z","shell.execute_reply":"2025-02-12T01:50:19.912451Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"distances.shape, indexes.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:50:36.011156Z","iopub.execute_input":"2025-02-12T01:50:36.011648Z","iopub.status.idle":"2025-02-12T01:50:36.018367Z","shell.execute_reply.started":"2025-02-12T01:50:36.011603Z","shell.execute_reply":"2025-02-12T01:50:36.017433Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"((60, 100), (60, 100))"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"indexes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:50:46.087398Z","iopub.execute_input":"2025-02-12T01:50:46.087712Z","iopub.status.idle":"2025-02-12T01:50:46.093828Z","shell.execute_reply.started":"2025-02-12T01:50:46.087687Z","shell.execute_reply":"2025-02-12T01:50:46.092786Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"array([[  5716,  12198,   5423, ...,  59100, 114094,  63774],\n       [  5837,   5572,   5676, ...,  21965, 107317,  70830],\n       [  5423,   5716,  18189, ...,  91724, 150225,  86091],\n       ...,\n       [  5565,   5566,   5567, ...,  71781,  43511,  21493],\n       [  5423,   5716,   5676, ...,   5427,  59100,  63774],\n       [  5472,   5473,   5476, ..., 105172,  95016, 124645]])"},"metadata":{}}],"execution_count":62},{"cell_type":"markdown","source":"# Appendix","metadata":{}},{"cell_type":"code","source":"content.document_group.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k,v in content[content.document_group == 'docs'].iloc[0].to_dict().items():\n    print(k,v)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l = list(content[content.document_group == 'docs'][content['raw_text'].notna()]['raw_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lengths = sorted(list(map(len, l)))[:-100]\nsum(lengths)/len(l), max(lengths), min(lengths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(lengths[:-1000])\nplt.title(\"histogram of number of documents with given length\")\nplt.xlabel(\"document length\")\nplt.ylabel(\"# of documents\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k,v in content[content.document_group == 'blog'].iloc[100].to_dict().items():\n    print(k,v)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l = list(content[content.document_group == 'blog'][content['raw_text'].notna()]['raw_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lengths = sorted(list(map(len, l)))\nsum(lengths)/len(l), max(lengths), min(lengths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(lengths[:-1000])\nplt.title(\"histogram of number of documents with given length\")\nplt.xlabel(\"document length\")\nplt.ylabel(\"# of documents\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Queries EDA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nqueries = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/test.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"queries = list(queries['Query'])\nlen(queries)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from random import random\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for _ in range(20):\n    print(queries[int(random() * len(queries))])\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfqueries = filter(lambda text: ('how to' not in text) and ('what is' not in text), queries)\nhow_to = filter(lambda text: 'how to' in text, queries)\nwhat_is = filter(lambda text: 'what is' in text, queries)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stats(fqueries):\n    fqueries = list(fqueries)\n    print(len(fqueries), len(fqueries)/len(queries))\n    for _ in range(20):\n        print(fqueries[int(random() * len(fqueries) - 1)])\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stats(fqueries)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stats(how_to)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stats(what_is)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/sample_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(sample_submission['DocumentId'])[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}