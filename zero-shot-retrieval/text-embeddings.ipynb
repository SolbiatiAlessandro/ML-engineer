{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":35704,"databundleVersionId":3515347,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alessandrosolbiati/zero-shot-learning-vmware-text-embedding?scriptVersionId=222080170\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:20:44.446382Z","iopub.execute_input":"2025-02-12T01:20:44.446725Z","iopub.status.idle":"2025-02-12T01:20:44.765025Z","shell.execute_reply.started":"2025-02-12T01:20:44.446696Z","shell.execute_reply":"2025-02-12T01:20:44.764083Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vmware-zero-shot-information-retrieval/sample_submission.csv\n/kaggle/input/vmware-zero-shot-information-retrieval/vmware_ir_content.csv\n/kaggle/input/vmware-zero-shot-information-retrieval/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"hyperparameters = {\n    'chunk_size_characters': 400,\n    'chunk_size_padding': 50,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:20:44.829513Z","iopub.execute_input":"2025-02-12T01:20:44.829881Z","iopub.status.idle":"2025-02-12T01:20:44.834097Z","shell.execute_reply.started":"2025-02-12T01:20:44.829856Z","shell.execute_reply":"2025-02-12T01:20:44.83285Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Chunk Documents","metadata":{"execution":{"iopub.status.busy":"2025-02-11T01:14:53.820782Z","iopub.execute_input":"2025-02-11T01:14:53.821095Z","iopub.status.idle":"2025-02-11T01:14:53.82444Z","shell.execute_reply.started":"2025-02-11T01:14:53.821075Z","shell.execute_reply":"2025-02-11T01:14:53.823436Z"}}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/test.csv\")\nsorted(list(test.Query), key=len)[1000:1100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pp = filter(lambda x: 'network' in x, list(test.Query))\nfor p in pp: print(p)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ncontent = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/vmware_ir_content.csv\")\nlen(content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:20:55.882679Z","iopub.execute_input":"2025-02-12T01:20:55.883031Z","iopub.status.idle":"2025-02-12T01:21:07.185242Z","shell.execute_reply.started":"2025-02-12T01:20:55.883005Z","shell.execute_reply":"2025-02-12T01:21:07.18448Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"323963"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"content = content[content.raw_text.fillna(\"\").apply(lambda x: 'network' in x)]\nlen(content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:07.186203Z","iopub.execute_input":"2025-02-12T01:21:07.186506Z","iopub.status.idle":"2025-02-12T01:21:07.952822Z","shell.execute_reply.started":"2025-02-12T01:21:07.186481Z","shell.execute_reply":"2025-02-12T01:21:07.952062Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"62371"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"content['raw_text'].str.len().sum() # 750M\n# num of chunks 750M / 20 = 30M ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:07.956743Z","iopub.execute_input":"2025-02-12T01:21:07.956993Z","iopub.status.idle":"2025-02-12T01:21:08.017166Z","shell.execute_reply.started":"2025-02-12T01:21:07.95696Z","shell.execute_reply":"2025-02-12T01:21:08.016469Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"275640277"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"sampled_content = content.sample(frac=0.01)\nlen(sampled_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:08.018165Z","iopub.execute_input":"2025-02-12T01:21:08.018492Z","iopub.status.idle":"2025-02-12T01:21:08.025738Z","shell.execute_reply.started":"2025-02-12T01:21:08.018469Z","shell.execute_reply":"2025-02-12T01:21:08.024953Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"624"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from dataclasses import dataclass\n\nchunk_size = hyperparameters['chunk_size_characters']\npadding = hyperparameters['chunk_size_padding']\nCHUNK_MAX_LENGTH = chunk_size + 2 * padding\n\ndef chunkify(document):\n    ix, chunks = 0, []\n    while ix < len(document):\n        chunk_start = ix \n        chunk_end   = ix + padding + chunk_size + padding\n        chunks.append(document[chunk_start:chunk_end])\n        ix = ix + padding + chunk_size\n    return chunks\n\n@dataclass\nclass Chunk:\n    document_id: int\n    chunk_data: str\n\nchunks = [] # id: chunk\nfor ix, (k,v) in enumerate(list(content['raw_text'].fillna(\"\").items())):\n\n    kchunks = chunkify(v)\n    for kchunk in kchunks:\n        if \"network\" in kchunk:\n            chunks.append(Chunk(document_id=k, chunk_data=kchunk))\n    if ix % 10000 == 0:\n        print(f\"id: {k}, passage length: {len(v)}, number of chunks: {len(kchunks)}\")\nprint(len(chunks))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:08.026543Z","iopub.execute_input":"2025-02-12T01:21:08.026844Z","iopub.status.idle":"2025-02-12T01:21:09.106555Z","shell.execute_reply.started":"2025-02-12T01:21:08.026815Z","shell.execute_reply":"2025-02-12T01:21:09.10549Z"}},"outputs":[{"name":"stdout","text":"id: 3, passage length: 3918, number of chunks: 9\nid: 49212, passage length: 1915, number of chunks: 5\nid: 104920, passage length: 1252, number of chunks: 3\nid: 157021, passage length: 3230, number of chunks: 8\nid: 208526, passage length: 3191, number of chunks: 8\nid: 259705, passage length: 1762, number of chunks: 4\nid: 311489, passage length: 4123, number of chunks: 10\n151101\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"c1 = chunks[0].chunk_data\nc2 = chunks[30].chunk_data\nc1, c2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Embedding model","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:12.592939Z","iopub.execute_input":"2025-02-12T01:21:12.593329Z","iopub.status.idle":"2025-02-12T01:21:14.192915Z","shell.execute_reply.started":"2025-02-12T01:21:12.593302Z","shell.execute_reply":"2025-02-12T01:21:14.191883Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = [f'query: {chunks[0].chunk_data}',\n               f'query: {chunks[-10].chunk_data}',\n               f'query: {chunks[1].chunk_data}',\n               f'query: {chunks[-11].chunk_data}']\ninput_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-small-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-small-v2').to(device)\n#  model alone on GPU T4 is 243MiB /  15360MiB \n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\ndef model_predict(input_texts: list[str], model=model, tokenizer=tokenizer):\n    # Tokenize the input texts\n    batch_dict = tokenizer(input_texts, max_length=CHUNK_MAX_LENGTH, padding=True, truncation=True, return_tensors='pt')\n    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n    with torch.no_grad():\n        outputs = model(**batch_dict)\n        embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n        # normalize embeddings\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n    return embeddings, batch_dict\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:21:50.902043Z","iopub.execute_input":"2025-02-12T01:21:50.902627Z","iopub.status.idle":"2025-02-12T01:21:56.380104Z","shell.execute_reply.started":"2025-02-12T01:21:50.902597Z","shell.execute_reply":"2025-02-12T01:21:56.379354Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nembeddings, _ = model_predict(input_texts)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunked_corpus = [f\"query: {chunk.chunk_data}\" for chunk in chunks][:3000]\nlen(chunked_corpus)\n\n# 1315MiB /  15360MiB after first inference of 300\n# 2709MiB /  15360MiB inference of 600\n# 13913MiB /  15360MiB  inference for 3000 chunks, takes 3 seconds (3 * 10^3 seconds)\n\n# total number of chunks is 3*10^7 = 10^4 seconds to embed them all = 3 hours\n# I have two GPUs and I can do it in parallel and is going to take 1 hour\n\n# 3000 embeddings are 3000 * 348 emb size * 4bytes = 1MB in memory  = 10^6\n# I have 10GB memory 10^10 , can hold 3 * 10^3 * 10^4 = 3 10^7 embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum([len(c) for c in chunked_corpus])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from time import time\nstart_time = time()\nembeddings = model_predict(chunked_corpus)\nend_time = time()\nprint(f\"Model inference for {len(chunked_corpus)} chunks: {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(embeddings.shape)\nend_time = time()\nprint(f\"Model inference for {len(chunked_corpus)} chunks: {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:22:27.902172Z","iopub.execute_input":"2025-02-12T01:22:27.902541Z","iopub.status.idle":"2025-02-12T01:22:28.174078Z","shell.execute_reply.started":"2025-02-12T01:22:27.902508Z","shell.execute_reply":"2025-02-12T01:22:28.173122Z"}},"outputs":[{"name":"stdout","text":"Wed Feb 12 01:22:27 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   55C    P0             28W /   70W |     243MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   35C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass ChunkTextDataset(Dataset):\n    def __init__(self, chunks: list[Chunk]):\n        # Prepend \"query: \" to each chunk's text.\n        self.texts = [f\"query: {chunk.chunk_data}\" for chunk in chunks]\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return self.texts[idx]\n\n\ndataset = ChunkTextDataset(chunks)\ndataloader = DataLoader(dataset, batch_size=1000, shuffle=False)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:22:07.303625Z","iopub.execute_input":"2025-02-12T01:22:07.304301Z","iopub.status.idle":"2025-02-12T01:22:07.378155Z","shell.execute_reply.started":"2025-02-12T01:22:07.304265Z","shell.execute_reply":"2025-02-12T01:22:07.377427Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:22:22.966496Z","iopub.execute_input":"2025-02-12T01:22:22.966803Z","iopub.status.idle":"2025-02-12T01:22:22.970825Z","shell.execute_reply.started":"2025-02-12T01:22:22.966781Z","shell.execute_reply":"2025-02-12T01:22:22.969872Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"all_embeddings = []\nimport gc\n    \n# Disable gradients for inference.\nwith torch.no_grad():\n    for batch in dataloader:\n\n        from time import time\n        start_time = time()\n\n        # Each 'batch' is a list of strings (already with \"query: \" prepended).\n        embeddings, batch_dict = model_predict(batch)  # Assume this returns a tensor of shape [batch_size, embedding_dim]\n\n        end_time = time()\n        \n        torch.cuda.empty_cache()\n        print(f\"Processed batch of {len(batch)} chunks, {end_time - start_time:.2f} seconds\")\n\n        # Move embeddings to CPU (if they are on GPU) and append.\n        all_embeddings.append(embeddings.cpu())\n\n        # Clean up GPU memory:\n        del batch_dict\n        del embeddings\n        del batch\n        torch.cuda.empty_cache()\n        gc.collect()\n              \n        \n\n        \n        \n# Concatenate all the batch embeddings into a single tensor.\nall_embeddings = torch.cat(all_embeddings, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:23:02.962629Z","iopub.execute_input":"2025-02-12T01:23:02.962938Z","iopub.status.idle":"2025-02-12T01:36:56.297856Z","shell.execute_reply.started":"2025-02-12T01:23:02.962915Z","shell.execute_reply":"2025-02-12T01:36:56.296759Z"}},"outputs":[{"name":"stdout","text":"Processed batch of 1000 chunks, 0.24 seconds\nProcessed batch of 1000 chunks, 0.24 seconds\nProcessed batch of 1000 chunks, 0.24 seconds\nProcessed batch of 1000 chunks, 0.23 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.27 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.38 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.40 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.37 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.38 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.38 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.26 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.36 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.35 seconds\nProcessed batch of 1000 chunks, 0.31 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 1000 chunks, 0.30 seconds\nProcessed batch of 1000 chunks, 0.28 seconds\nProcessed batch of 1000 chunks, 0.34 seconds\nProcessed batch of 1000 chunks, 0.29 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.32 seconds\nProcessed batch of 1000 chunks, 0.33 seconds\nProcessed batch of 101 chunks, 0.29 seconds\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"len(chunks)\n\nimport pickle\n\n\n\n# Save the chunks list to disk\nwith open('/kaggle/working/chunks151101_11_Feb_A__(network).pkl', 'wb') as f:\n    pickle.dump(chunks, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:40:55.360727Z","iopub.execute_input":"2025-02-12T01:40:55.361159Z","iopub.status.idle":"2025-02-12T01:40:55.73333Z","shell.execute_reply.started":"2025-02-12T01:40:55.361125Z","shell.execute_reply":"2025-02-12T01:40:55.732257Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"all_embeddings.shape\n\nnp.save('/kaggle/working/embeddings151101x384_11_Feb_A__(network).npy', all_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T01:39:34.939016Z","iopub.execute_input":"2025-02-12T01:39:34.939358Z","iopub.status.idle":"2025-02-12T01:39:35.09646Z","shell.execute_reply.started":"2025-02-12T01:39:34.939329Z","shell.execute_reply":"2025-02-12T01:39:35.095717Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"embeddings = all_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Indexing","metadata":{"execution":{"iopub.status.busy":"2025-02-12T01:42:42.270647Z","iopub.execute_input":"2025-02-12T01:42:42.27108Z","iopub.status.idle":"2025-02-12T01:42:42.274821Z","shell.execute_reply.started":"2025-02-12T01:42:42.271046Z","shell.execute_reply":"2025-02-12T01:42:42.274068Z"}}},{"cell_type":"code","source":"\n\n# Get the dimensionality of your embeddings.\n\n\ndimension = embeddings.shape[1]\n\n# Create a FAISS index. For example, use IndexFlatL2 for exact L2 (Euclidean) distance search:\nindex = faiss.IndexFlatL2(dimension)\n# For inner product similarity, you could use:\n# index = faiss.IndexFlatIP(dimension)\n\n# Add your embeddings to the index.\nindex.add(embeddings)\nprint(f\"Number of embeddings indexed: {index.ntotal}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Appendix","metadata":{}},{"cell_type":"code","source":"content.document_group.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k,v in content[content.document_group == 'docs'].iloc[0].to_dict().items():\n    print(k,v)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l = list(content[content.document_group == 'docs'][content['raw_text'].notna()]['raw_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lengths = sorted(list(map(len, l)))[:-100]\nsum(lengths)/len(l), max(lengths), min(lengths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(lengths[:-1000])\nplt.title(\"histogram of number of documents with given length\")\nplt.xlabel(\"document length\")\nplt.ylabel(\"# of documents\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k,v in content[content.document_group == 'blog'].iloc[100].to_dict().items():\n    print(k,v)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l = list(content[content.document_group == 'blog'][content['raw_text'].notna()]['raw_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lengths = sorted(list(map(len, l)))\nsum(lengths)/len(l), max(lengths), min(lengths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(lengths[:-1000])\nplt.title(\"histogram of number of documents with given length\")\nplt.xlabel(\"document length\")\nplt.ylabel(\"# of documents\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Queries EDA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nqueries = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/test.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"queries = list(queries['Query'])\nlen(queries)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from random import random\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for _ in range(20):\n    print(queries[int(random() * len(queries))])\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfqueries = filter(lambda text: ('how to' not in text) and ('what is' not in text), queries)\nhow_to = filter(lambda text: 'how to' in text, queries)\nwhat_is = filter(lambda text: 'what is' in text, queries)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stats(fqueries):\n    fqueries = list(fqueries)\n    print(len(fqueries), len(fqueries)/len(queries))\n    for _ in range(20):\n        print(fqueries[int(random() * len(fqueries) - 1)])\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stats(fqueries)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stats(how_to)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stats(what_is)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/vmware-zero-shot-information-retrieval/sample_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(sample_submission['DocumentId'])[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}